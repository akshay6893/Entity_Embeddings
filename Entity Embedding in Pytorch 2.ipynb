{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# This is a data set about the number of bikes shared or rented in each month.  \n",
    "import pandas as pd \n",
    "bike = pd.read_csv(\"bike.csv\")\n",
    "sub_bike = bike.loc[:,(\"mnth\",\"cnt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "\n",
    "#   The number of bikes bikes rented ranges from 22 to 8714.To bring it between 0 and 1 divide it by 8714 and \n",
    "#   converting it to a Tensor. \n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "list1 = sub_bike[\"cnt\"].tolist()\n",
    "wow = [t/8714 for t in list1 ]\n",
    "n_list = partial(round,ndigits=4)\n",
    "normalised1 = list(map(n_list,wow))\n",
    "normalised2 = np.asarray(normalised1)\n",
    "normalised3 = normalised2.reshape(731,1)\n",
    "normalised_lables = torch.Tensor(normalised3)\n",
    "\n",
    "# Taking the one hot encoding of the month column to multiply in the embedding matrix and converting to Tensor. \n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import OneHotEncoder as ohr\n",
    "OH = ohr(sparse=False)\n",
    "OHE1 = OH.fit_transform(sub_bike[[\"mnth\"]])\n",
    "OHE = torch.Tensor(OHE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net1(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(12,3)\n",
    "        self.fc1 = nn.Linear(3,50)\n",
    "        self.fc2 = nn.Linear(50,15)\n",
    "        self.fc3 = nn.Linear(15,1)\n",
    "        \n",
    "    def forward(self,t):\n",
    "                              # Embedding\n",
    "        t=t \n",
    "        u = self.fc(t) \n",
    "                              # Hidden Layer 3,50\n",
    "        t = self.fc1(u)\n",
    "        #t = F.dropout(t)\n",
    "        t = torch.relu(t)\n",
    "\n",
    "                              # Hidden layer 50, 15\n",
    "        t = self.fc2(t)\n",
    "        #t = F.dropout(t)\n",
    "        t = torch.relu(t)\n",
    "\n",
    "                              # For converting 15 to 1 \n",
    "        t = self.fc3(t)\n",
    "        #t = torch.sigmoid(t)\n",
    "        \n",
    "        return t\n",
    "    \n",
    "\n",
    "    \n",
    "layer = Net1()\n",
    "\n",
    "layer(OHE[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim as optim\n",
    "optimiser = optim.Adam(layer.parameters(),lr=0.001)\n",
    "batch_OHE = torch.utils.data.DataLoader(OHE,batch_size=40)\n",
    "batch_lables = torch.utils.data.DataLoader(normalised_lables,batch_size=40)\n",
    "\n",
    "# for epoch in range(200):\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    total_loss = 0\n",
    "    pair = []\n",
    "    for i,j in zip(batch_OHE,batch_lables):\n",
    "                \n",
    "        pred = layer(i)\n",
    "        \n",
    "        loss = F.mse_loss(pred,j)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "        total_loss +=  loss.item()\n",
    "        pair.append(i)\n",
    "        pair.append(j)\n",
    "    print (\"epoch\",epoch,\"loss:\",total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a 3D graph and it is expected that months 5,6,7,8,9 should be grouped together and 1,2 together. \n",
    "\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from numpy.random import rand\n",
    "from pylab import figure\n",
    "%matplotlib inline  \n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "#m=rand(3,3) m is an array of (x,y,z) coordinate triplets\n",
    "\n",
    "fig = figure()\n",
    "ax = Axes3D(fig)\n",
    "fg1 = layer.fc.weight\n",
    "fg2 = fg1.detach().numpy().reshape(12,3)\n",
    "\n",
    "\n",
    "for i in range(12):\n",
    "    #plot each point + it's index as text above\n",
    "    ax.scatter(fg2[i,0],fg2[i,1],fg2[i,2],color='b') \n",
    "    ax.text(fg2[i,0],fg2[i,1],fg2[i,2],  '%s' % (str(i)), size=15, zorder=1,color='r') \n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i expect the months with similar sales to learn embeddings which are close together.\n",
    "\n",
    "group_sub_bike = sub_bike.groupby([\"mnth\"]).sum()\n",
    "months3 = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "group_sub_bike.insert(1,\"months3\",months3,True)\n",
    "group_sub_bike.plot.bar(\"months3\",\"cnt\",rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
